{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gBkxsrVHmWLt"
   },
   "source": [
    "# seq2seq model for machine translation from English to Italian.\n",
    "\n",
    "### - Malav Shastri\n",
    "\n",
    "### Task: Translate English to Italian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TydBPfYtmWLw"
   },
   "source": [
    "### Notes: \n",
    "\n",
    "To implement ```Bi-LSTM```, you will need the following code to build the encoder. Do NOT use Bi-LSTM for the decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SwECndeBmWLx"
   },
   "outputs": [],
   "source": [
    "#from keras.layers import Bidirectional, Concatenate\n",
    "\n",
    "#encoder_bilstm = Bidirectional(LSTM(latent_dim, return_state=True, \n",
    "                                 # dropout=0.5, name='encoder_lstm'))\n",
    "#_, forward_h, forward_c, backward_h, backward_c = encoder_bilstm(encoder_inputs)\n",
    "\n",
    "#state_h = Concatenate()([forward_h, backward_h])\n",
    "#state_c = Concatenate()([forward_c, backward_c])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "grh6CG9dmWL2"
   },
   "source": [
    "## 1. Data preparation\n",
    "\n",
    "1. Download data (e.g., \"deu-eng.zip\") from http://www.manythings.org/anki/\n",
    "2. Unzip the .ZIP file.\n",
    "3. Put the .TXT file (e.g., \"deu.txt\") in the directory \"./Data/\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pfPLQ6BsmWL3"
   },
   "source": [
    "### 1.1. Load and clean text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "adNqypoSmWL4"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from unicodedata import normalize\n",
    "import numpy\n",
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, mode='rt', encoding='utf-8')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "\n",
    "# split a loaded document into sentences\n",
    "def to_pairs(doc):\n",
    "    lines = doc.strip().split('\\n')\n",
    "    pairs = [line.split('\\t') for line in  lines]\n",
    "    return pairs\n",
    "\n",
    "def clean_data(lines):\n",
    "    cleaned = list()\n",
    "    # prepare regex for char filtering\n",
    "    re_print = re.compile('[^%s]' % re.escape(string.printable))\n",
    "    # prepare translation table for removing punctuation\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    for pair in lines:\n",
    "        clean_pair = list()\n",
    "        for line in pair:\n",
    "            # normalize unicode characters\n",
    "            line = normalize('NFD', line).encode('ascii', 'ignore')\n",
    "            line = line.decode('UTF-8')\n",
    "            # tokenize on white space\n",
    "            line = line.split()\n",
    "            # convert to lowercase\n",
    "            line = [word.lower() for word in line]\n",
    "            # remove punctuation from each token\n",
    "            line = [word.translate(table) for word in line]\n",
    "            # remove non-printable chars form each token\n",
    "            line = [re_print.sub('', w) for w in line]\n",
    "            # remove tokens with numbers in them\n",
    "            line = [word for word in line if word.isalpha()]\n",
    "            # store as string\n",
    "            clean_pair.append(' '.join(line))\n",
    "        cleaned.append(clean_pair)\n",
    "    return numpy.array(cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IEZOIyzzmWL8"
   },
   "outputs": [],
   "source": [
    "# e.g., filename = 'Data/deu.txt'\n",
    "filename = 'ita.txt'\n",
    "\n",
    "# e.g., n_train = 20000\n",
    "n_train = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "gakRbJFSmWL_",
    "outputId": "a6b925f7-41eb-446a-d1be-93998f767e09"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "336614\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "doc = load_doc(filename)\n",
    "\n",
    "# split into Language1-Language2 pairs\n",
    "pairs = to_pairs(doc)\n",
    "print(len(pairs))\n",
    "\n",
    "# clean sentences\n",
    "clean_pairs = clean_data(pairs)[0:n_train, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "QSm3bLUDmWMC",
    "outputId": "16efe48d-684e-4ded-aeb6-5f8ec7a46f7f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[im better] => [io sono meglio]\n",
      "[im better] => [sono migliore]\n",
      "[im better] => [io sono migliore]\n",
      "[im better] => [sto meglio]\n",
      "[im better] => [io sto meglio]\n",
      "[im biased] => [io sono di parte]\n",
      "[im biased] => [sono tendenzioso]\n",
      "[im biased] => [io sono tendenzioso]\n",
      "[im biased] => [sono tendenziosa]\n",
      "[im biased] => [io sono tendenziosa]\n"
     ]
    }
   ],
   "source": [
    "for i in range(3000, 3010):\n",
    "    print('[' + clean_pairs[i, 0] + '] => [' + clean_pairs[i, 1] + ']')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "48H1bhhFmWMF",
    "outputId": "142d66a3-b19e-44f5-83ab-a256b244c42c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of input_texts:  (20000,)\n",
      "Length of target_texts: (20000,)\n"
     ]
    }
   ],
   "source": [
    "input_texts = clean_pairs[:, 0]\n",
    "target_texts = ['\\t' + text + '\\n' for text in clean_pairs[:, 1]]\n",
    "\n",
    "print('Length of input_texts:  ' + str(input_texts.shape))\n",
    "print('Length of target_texts: ' + str(input_texts.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "NAY4WTjNmWMG",
    "outputId": "576e1a30-fc46-4552-886a-4f62445c3023"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max length of input  sentences: 14\n",
      "max length of target sentences: 41\n"
     ]
    }
   ],
   "source": [
    "max_encoder_seq_length = max(len(line) for line in input_texts)\n",
    "max_decoder_seq_length = max(len(line) for line in target_texts)\n",
    "\n",
    "print('max length of input  sentences: %d' % (max_encoder_seq_length))\n",
    "print('max length of target sentences: %d' % (max_decoder_seq_length))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qIRQDttPmWMI"
   },
   "source": [
    "**Remark:** To this end, you have two lists of sentences: input_texts and target_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rViClxnZmWMJ"
   },
   "source": [
    "## 2. Text processing\n",
    "\n",
    "### 2.1. Convert texts to sequences\n",
    "\n",
    "- Input: A list of $n$ sentences (with max length $t$).\n",
    "- It is represented by a $n\\times t$ matrix after the tokenization and zero-padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "szqMyagzmWMJ",
    "outputId": "afb0ccaf-756f-43dc-fc1b-c7837a279887"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of encoder_input_seq: (20000, 14)\n",
      "shape of input_token_index: 27\n",
      "shape of decoder_input_seq: (20000, 41)\n",
      "shape of target_token_index: 29\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# encode and pad sequences\n",
    "def text2sequences(max_len, lines):\n",
    "    tokenizer = Tokenizer(char_level=True, filters='')\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    seqs = tokenizer.texts_to_sequences(lines)\n",
    "    seqs_pad = pad_sequences(seqs, maxlen=max_len, padding='post')\n",
    "    return seqs_pad, tokenizer.word_index\n",
    "\n",
    "\n",
    "encoder_input_seq, input_token_index = text2sequences(max_encoder_seq_length, \n",
    "                                                      input_texts)\n",
    "decoder_input_seq, target_token_index = text2sequences(max_decoder_seq_length, \n",
    "                                                       target_texts)\n",
    "\n",
    "print('shape of encoder_input_seq: ' + str(encoder_input_seq.shape))\n",
    "print('shape of input_token_index: ' + str(len(input_token_index)))\n",
    "print('shape of decoder_input_seq: ' + str(decoder_input_seq.shape))\n",
    "print('shape of target_token_index: ' + str(len(target_token_index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "G1g-IwABmWML",
    "outputId": "374bedd7-9e61-465b-c629-e0539f8b01c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_encoder_tokens: 28\n",
      "num_decoder_tokens: 30\n"
     ]
    }
   ],
   "source": [
    "num_encoder_tokens = len(input_token_index) + 1\n",
    "num_decoder_tokens = len(target_token_index) + 1\n",
    "\n",
    "print('num_encoder_tokens: ' + str(num_encoder_tokens))\n",
    "print('num_decoder_tokens: ' + str(num_decoder_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G9_mgbSwmWMM"
   },
   "source": [
    "**Remark:** To this end, the input language and target language texts are converted to 2 matrices. \n",
    "\n",
    "- Their number of rows are both n_train.\n",
    "- Their number of columns are respective max_encoder_seq_length and max_decoder_seq_length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tbrTSYvjmWMN"
   },
   "source": [
    "The followings print a sentence and its representation as a sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "gJJ7E6sPmWMN",
    "outputId": "e91879fb-e728-48fc-9810-3a4ef7fafa22"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\tio lavoro a maglia\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_texts[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "R_LXqgq_mWMP",
    "outputId": "769a072f-d50b-46ab-86ff-b06fb0d4891c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6,  4,  1,  2, 12,  3, 18,  1, 11,  1,  2,  3,  2, 13,  3, 19, 12,\n",
       "        4,  3,  7,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0], dtype=int32)"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_input_seq[100, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qJvViC1PmWMQ"
   },
   "source": [
    "## 2.2. One-hot encode\n",
    "\n",
    "- Input: A list of $n$ sentences (with max length $t$).\n",
    "- It is represented by a $n\\times t$ matrix after the tokenization and zero-padding.\n",
    "- It is represented by a $n\\times t \\times v$ tensor ($t$ is the number of unique chars) after the one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "ki6mU6CVmWMQ",
    "outputId": "f0604e14-cf0e-413c-d8e1-5833f622cd10"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 14, 28)\n",
      "(20000, 41, 30)\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "# one hot encode target sequence\n",
    "def onehot_encode(sequences, max_len, vocab_size):\n",
    "    n = len(sequences)\n",
    "    data = numpy.zeros((n, max_len, vocab_size))\n",
    "    for i in range(n):\n",
    "        data[i, :, :] = to_categorical(sequences[i], num_classes=vocab_size)\n",
    "    return data\n",
    "\n",
    "encoder_input_data = onehot_encode(encoder_input_seq, max_encoder_seq_length, num_encoder_tokens)\n",
    "decoder_input_data = onehot_encode(decoder_input_seq, max_decoder_seq_length, num_decoder_tokens)\n",
    "\n",
    "decoder_target_seq = numpy.zeros(decoder_input_seq.shape)\n",
    "decoder_target_seq[:, 0:-1] = decoder_input_seq[:, 1:]\n",
    "decoder_target_data = onehot_encode(decoder_target_seq, \n",
    "                                    max_decoder_seq_length, \n",
    "                                    num_decoder_tokens)\n",
    "\n",
    "print(encoder_input_data.shape)\n",
    "print(decoder_input_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vs8UHmXqmWMS"
   },
   "source": [
    "## 3. Build the networks (for training)\n",
    "\n",
    "- Build encoder, decoder, and connect the two modules to get \"model\". \n",
    "\n",
    "- Fit the model on the bilingual data to train the parameters in the encoder and decoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w44HVEPsmWMS"
   },
   "source": [
    "### 3.1. Encoder network\n",
    "\n",
    "- Input:  one-hot encode of the input language\n",
    "\n",
    "- Return: \n",
    "\n",
    "    -- output (all the hidden states   $h_1, \\cdots , h_t$) are always discarded\n",
    "    \n",
    "    -- the final hidden state  $h_t$\n",
    "    \n",
    "    -- the final conveyor belt $c_t$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ov3vcvVHmWMS"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, LSTM\n",
    "from keras.models import Model\n",
    "from keras.layers import Bidirectional, Concatenate\n",
    "\n",
    "latent_dim = 256\n",
    "\n",
    "# inputs of the encoder network\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens), \n",
    "                       name='encoder_inputs')\n",
    "\n",
    "# set the biLSTM layer\n",
    "encoder_bilstm = Bidirectional(LSTM(latent_dim, return_state=True, dropout=0.5, name='encoder_lstm'))\n",
    "_, forward_h, forward_c, backward_h, backward_c = encoder_bilstm(encoder_inputs)\n",
    "\n",
    "state_h = Concatenate()([forward_h, backward_h])\n",
    "state_c = Concatenate()([forward_c, backward_c])\n",
    "\n",
    "# build the encoder network model\n",
    "encoder_model = Model(inputs=encoder_inputs, \n",
    "                      outputs=[state_h, state_c],\n",
    "                      name='encoder')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WM0odUv0mWMU"
   },
   "source": [
    "Print a summary and save the encoder network structure to \"./encoder.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 323
    },
    "colab_type": "code",
    "id": "zv2fdoyHmWMU",
    "outputId": "7b5dc88d-6641-4cf1-9435-7189d80c9081"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_inputs (InputLayer)     (None, None, 28)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) [(None, 512), (None, 583680      encoder_inputs[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 512)          0           bidirectional_1[0][1]            \n",
      "                                                                 bidirectional_1[0][3]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 512)          0           bidirectional_1[0][2]            \n",
      "                                                                 bidirectional_1[0][4]            \n",
      "==================================================================================================\n",
      "Total params: 583,680\n",
      "Trainable params: 583,680\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot, plot_model\n",
    "\n",
    "SVG(model_to_dot(encoder_model, show_shapes=False).create(prog='dot', format='svg'))\n",
    "\n",
    "plot_model(\n",
    "    model=encoder_model, show_shapes=False,\n",
    "    to_file='encoder.pdf'\n",
    ")\n",
    "\n",
    "encoder_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aqpmaV8_mWMV"
   },
   "source": [
    "### 3.2. Decoder network\n",
    "\n",
    "- Inputs:  \n",
    "\n",
    "    -- one-hot encode of the target language\n",
    "    \n",
    "    -- The initial hidden state $h_t$ \n",
    "    \n",
    "    -- The initial conveyor belt $c_t$ \n",
    "\n",
    "- Return: \n",
    "\n",
    "    -- output (all the hidden states) $h_1, \\cdots , h_t$\n",
    "\n",
    "    -- the final hidden state  $h_t$ (discarded in the training and used in the prediction)\n",
    "    \n",
    "    -- the final conveyor belt $c_t$ (discarded in the training and used in the prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mm06KWh3mWMV"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, LSTM, Dense\n",
    "from keras.models import Model\n",
    "\n",
    "# inputs of the decoder network\n",
    "decoder_input_h = Input(shape=(latent_dim*2,), name='decoder_input_h')\n",
    "decoder_input_c = Input(shape=(latent_dim*2,), name='decoder_input_c')\n",
    "decoder_input_x = Input(shape=(None, num_decoder_tokens), name='decoder_input_x')\n",
    "\n",
    "# set the LSTM layer\n",
    "decoder_lstm = LSTM(latent_dim*2, return_sequences=True, \n",
    "                    return_state=True, dropout=0.5, name='decoder_lstm')\n",
    "decoder_lstm_outputs, state_h, state_c = decoder_lstm(decoder_input_x, \n",
    "                                                      initial_state=[decoder_input_h, decoder_input_c])\n",
    "\n",
    "# set the dense layer\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax', name='decoder_dense')\n",
    "decoder_outputs = decoder_dense(decoder_lstm_outputs)\n",
    "\n",
    "# build the decoder network model\n",
    "decoder_model = Model(inputs=[decoder_input_x, decoder_input_h, decoder_input_c],\n",
    "                      outputs=[decoder_outputs, state_h, state_c],\n",
    "                      name='decoder')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dtU1SxD3mWMX"
   },
   "source": [
    "Print a summary and save the encoder network structure to \"./decoder.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "colab_type": "code",
    "id": "n6Tnp55VmWMX",
    "outputId": "b89ce0a1-9444-4837-e154-792685194ca5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"decoder\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "decoder_input_x (InputLayer)    (None, None, 30)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_input_h (InputLayer)    (None, 512)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_input_c (InputLayer)    (None, 512)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_lstm (LSTM)             [(None, None, 512),  1112064     decoder_input_x[0][0]            \n",
      "                                                                 decoder_input_h[0][0]            \n",
      "                                                                 decoder_input_c[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_dense (Dense)           (None, None, 30)     15390       decoder_lstm[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 1,127,454\n",
      "Trainable params: 1,127,454\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot, plot_model\n",
    "\n",
    "SVG(model_to_dot(decoder_model, show_shapes=False).create(prog='dot', format='svg'))\n",
    "\n",
    "plot_model(\n",
    "    model=decoder_model, show_shapes=False,\n",
    "    to_file='decoder.pdf'\n",
    ")\n",
    "\n",
    "decoder_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RoaF2RLlmWMZ"
   },
   "source": [
    "### 3.3. Connect the encoder and decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yhcvLLrXmWMZ"
   },
   "outputs": [],
   "source": [
    "# input layers\n",
    "encoder_input_x = Input(shape=(None, num_encoder_tokens), name='encoder_input_x')\n",
    "decoder_input_x = Input(shape=(None, num_decoder_tokens), name='decoder_input_x')\n",
    "\n",
    "# connect encoder to decoder\n",
    "encoder_final_states = encoder_model([encoder_input_x])\n",
    "decoder_lstm_output, _, _ = decoder_lstm(decoder_input_x, initial_state=encoder_final_states)\n",
    "decoder_pred = decoder_dense(decoder_lstm_output)\n",
    "\n",
    "model = Model(inputs=[encoder_input_x, decoder_input_x], \n",
    "              outputs=decoder_pred, \n",
    "              name='model_training')\n",
    "\n",
    "\n",
    "# input layers\n",
    "encoder_input_x = Input(shape=(None, num_encoder_tokens), name='encoder_input_x')\n",
    "decoder_input_x = Input(shape=(None, num_decoder_tokens), name='decoder_input_x')\n",
    "\n",
    "# connect encoder to decoder\n",
    "encoder_final_states = encoder_model([encoder_input_x])\n",
    "decoder_lstm_output, _, _ = decoder_lstm(decoder_input_x, initial_state=encoder_final_states)\n",
    "decoder_pred = decoder_dense(decoder_lstm_output)\n",
    "\n",
    "model = Model(inputs=[encoder_input_x, decoder_input_x], \n",
    "              outputs=decoder_pred, \n",
    "              name='model_training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "q2WnQnDOmWMb",
    "outputId": "c6fafb6b-f193-4cd5-e1cf-2ee66d16b196"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"decoder_lstm/while:4\", shape=(None, 512), dtype=float32)\n",
      "Tensor(\"decoder_input_h:0\", shape=(None, 512), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(state_h)\n",
    "print(decoder_input_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "colab_type": "code",
    "id": "xmYNXjncmWMc",
    "outputId": "52fd355d-4e4d-4a0e-af6f-86ba32e17ad3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_training\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_input_x (InputLayer)    (None, None, 28)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_input_x (InputLayer)    (None, None, 30)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder (Model)                 [(None, 512), (None, 583680      encoder_input_x[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_lstm (LSTM)             [(None, None, 512),  1112064     decoder_input_x[0][0]            \n",
      "                                                                 encoder[2][0]                    \n",
      "                                                                 encoder[2][1]                    \n",
      "__________________________________________________________________________________________________\n",
      "decoder_dense (Dense)           (None, None, 30)     15390       decoder_lstm[2][0]               \n",
      "==================================================================================================\n",
      "Total params: 1,711,134\n",
      "Trainable params: 1,711,134\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot, plot_model\n",
    "\n",
    "SVG(model_to_dot(model, show_shapes=False).create(prog='dot', format='svg'))\n",
    "\n",
    "plot_model(\n",
    "    model=model, show_shapes=False,\n",
    "    to_file='model_training.pdf'\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d-KmPT3bmWMd"
   },
   "source": [
    "### 3.5. Fit the model on the bilingual dataset\n",
    "\n",
    "- encoder_input_data: one-hot encode of the input language\n",
    "\n",
    "- decoder_input_data: one-hot encode of the input language\n",
    "\n",
    "- decoder_target_data: labels (left shift of decoder_input_data)\n",
    "\n",
    "- tune the hyper-parameters\n",
    "\n",
    "- stop when the validation loss stop decreasing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "0-D2OJk8mWMe",
    "outputId": "35a6acfa-b52b-4186-d43a-0a599ed6b634"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of encoder_input_data(20000, 14, 28)\n",
      "shape of decoder_input_data(20000, 41, 30)\n",
      "shape of decoder_target_data(20000, 41, 30)\n"
     ]
    }
   ],
   "source": [
    "print('shape of encoder_input_data' + str(encoder_input_data.shape))\n",
    "print('shape of decoder_input_data' + str(decoder_input_data.shape))\n",
    "print('shape of decoder_target_data' + str(decoder_target_data.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 714
    },
    "colab_type": "code",
    "id": "iuGzED77mWMf",
    "outputId": "6d2aaa25-6956-4027-b29d-4b6444dd3564"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/20\n",
      "16000/16000 [==============================] - 32s 2ms/step - loss: 1.1476 - val_loss: 0.9496\n",
      "Epoch 2/20\n",
      "16000/16000 [==============================] - 31s 2ms/step - loss: 0.8817 - val_loss: 0.8387\n",
      "Epoch 3/20\n",
      "16000/16000 [==============================] - 30s 2ms/step - loss: 0.8158 - val_loss: 0.7745\n",
      "Epoch 4/20\n",
      "16000/16000 [==============================] - 31s 2ms/step - loss: 0.7709 - val_loss: 0.7260\n",
      "Epoch 5/20\n",
      "16000/16000 [==============================] - 31s 2ms/step - loss: 0.7359 - val_loss: 0.6894\n",
      "Epoch 6/20\n",
      "16000/16000 [==============================] - 31s 2ms/step - loss: 0.7073 - val_loss: 0.6598\n",
      "Epoch 7/20\n",
      "16000/16000 [==============================] - 31s 2ms/step - loss: 0.6834 - val_loss: 0.6492\n",
      "Epoch 8/20\n",
      "16000/16000 [==============================] - 31s 2ms/step - loss: 0.6637 - val_loss: 0.6277\n",
      "Epoch 9/20\n",
      "16000/16000 [==============================] - 31s 2ms/step - loss: 0.6429 - val_loss: 0.6203\n",
      "Epoch 10/20\n",
      "16000/16000 [==============================] - 31s 2ms/step - loss: 0.6256 - val_loss: 0.6040\n",
      "Epoch 11/20\n",
      "16000/16000 [==============================] - 33s 2ms/step - loss: 0.6096 - val_loss: 0.5948\n",
      "Epoch 12/20\n",
      "16000/16000 [==============================] - 32s 2ms/step - loss: 0.5991 - val_loss: 0.5844\n",
      "Epoch 13/20\n",
      "16000/16000 [==============================] - 31s 2ms/step - loss: 0.5849 - val_loss: 0.5838\n",
      "Epoch 14/20\n",
      "16000/16000 [==============================] - 30s 2ms/step - loss: 0.5702 - val_loss: 0.5716\n",
      "Epoch 15/20\n",
      "16000/16000 [==============================] - 30s 2ms/step - loss: 0.5599 - val_loss: 0.5737\n",
      "Epoch 16/20\n",
      "16000/16000 [==============================] - 30s 2ms/step - loss: 0.5484 - val_loss: 0.5637\n",
      "Epoch 17/20\n",
      "16000/16000 [==============================] - 30s 2ms/step - loss: 0.5388 - val_loss: 0.5582\n",
      "Epoch 18/20\n",
      "16000/16000 [==============================] - 30s 2ms/step - loss: 0.5294 - val_loss: 0.5534\n",
      "Epoch 19/20\n",
      "16000/16000 [==============================] - 29s 2ms/step - loss: 0.5180 - val_loss: 0.5543\n",
      "Epoch 20/20\n",
      "16000/16000 [==============================] - 29s 2ms/step - loss: 0.5124 - val_loss: 0.5495\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "\n",
    "model.fit([encoder_input_data, decoder_input_data],  # training data\n",
    "          decoder_target_data,                       # labels (left shift of the target sequences)\n",
    "          batch_size=64, epochs=20, validation_split=0.2)\n",
    "\n",
    "model.save('seq2seq.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BdMztpchmWMg"
   },
   "source": [
    "## 4. Make predictions\n",
    "\n",
    "\n",
    "### 4.1. Translate English to Italian\n",
    "\n",
    "1. Encoder read a sentence (source language) and output its final states, $h_t$ and $c_t$.\n",
    "2. Take the [star] sign \"\\t\" and the final state $h_t$ and $c_t$ as input and run the decoder.\n",
    "3. Get the new states and predicted probability distribution.\n",
    "4. sample a char from the predicted probability distribution\n",
    "5. take the sampled char and the new states as input and repeat the process (stop if reach the [stop] sign \"\\n\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O-x8xs0fmWMg"
   },
   "outputs": [],
   "source": [
    "# Reverse-lookup token index to decode sequences back to something readable.\n",
    "reverse_input_char_index = dict((i, char) for char, i in input_token_index.items())\n",
    "reverse_target_char_index = dict((i, char) for char, i in target_token_index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u9yLzsPqmWMh"
   },
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    target_seq = numpy.zeros((1, 1, num_decoder_tokens))\n",
    "    target_seq[0, 0, target_token_index['\\t']] = 1.\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        # this line of code is greedy selection\n",
    "        # try to use multinomial sampling instead (with temperature)\n",
    "        sampled_token_index = numpy.argmax(output_tokens[0, -1, :])\n",
    "        \n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "\n",
    "        target_seq = numpy.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "EbryaRtrmWMi",
    "outputId": "4e69813a-f5eb-4355-e2e7-2279562139d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "English:        tom voted\n",
      "Italian (true):  tom voto\n",
      "Italian (pred):  tom ha fatto un  passio\n",
      "-\n",
      "English:        tom walks\n",
      "Italian (true):  tom cammina\n",
      "Italian (pred):  tom ha patto un pansio\n",
      "-\n",
      "English:        tom walks\n",
      "Italian (true):  tom passeggia\n",
      "Italian (pred):  tom ha patto un pansio\n",
      "-\n",
      "English:        tom waved\n",
      "Italian (true):  tom ha salutato\n",
      "Italian (pred):  tom ha antito\n",
      "-\n",
      "English:        tom waved\n",
      "Italian (true):  tom saluto\n",
      "Italian (pred):  tom ha antito\n",
      "-\n",
      "English:        tom works\n",
      "Italian (true):  tom lavora\n",
      "Italian (pred):  tom ha riso\n",
      "-\n",
      "English:        tomll go\n",
      "Italian (true):  tom andra\n",
      "Italian (pred):  tom e divertente\n",
      "-\n",
      "English:        toms fat\n",
      "Italian (true):  tom e grasso\n",
      "Italian (pred):  tom e stato dicentiato\n",
      "-\n",
      "English:        toms mad\n",
      "Italian (true):  tom e matto\n",
      "Italian (pred):  tom e stanco\n",
      "-\n",
      "English:        toms sad\n",
      "Italian (true):  tom e triste\n",
      "Italian (pred):  tom e stato dicenziato\n",
      "-\n",
      "English:        toms shy\n",
      "Italian (true):  tom e timido\n",
      "Italian (pred):  tom e stanca\n",
      "-\n",
      "English:        trust tom\n",
      "Italian (true):  fidati di tom\n",
      "Italian (pred):  scrida di tom\n",
      "-\n",
      "English:        trust tom\n",
      "Italian (true):  fidatevi di tom\n",
      "Italian (pred):  scrida di tom\n",
      "-\n",
      "English:        trust tom\n",
      "Italian (true):  si fidi di tom\n",
      "Italian (pred):  scrida di tom\n",
      "-\n",
      "English:        try again\n",
      "Italian (true):  prova ancora\n",
      "Italian (pred):  provate di nuovo\n",
      "-\n",
      "English:        try again\n",
      "Italian (true):  prova di nuovo\n",
      "Italian (pred):  provate di nuovo\n",
      "-\n",
      "English:        try again\n",
      "Italian (true):  provi ancora\n",
      "Italian (pred):  provate di nuovo\n",
      "-\n",
      "English:        try again\n",
      "Italian (true):  provi di nuovo\n",
      "Italian (pred):  provate di nuovo\n",
      "-\n",
      "English:        try again\n",
      "Italian (true):  provate ancora\n",
      "Italian (pred):  provate di nuovo\n",
      "-\n",
      "English:        try again\n",
      "Italian (true):  provate di nuovo\n",
      "Italian (pred):  provate di nuovo\n"
     ]
    }
   ],
   "source": [
    "for seq_index in range(2100, 2120):\n",
    "    # Take one sequence (part of the training set)\n",
    "    # for trying out decoding.\n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print('-')\n",
    "    print('English:       ', input_texts[seq_index])\n",
    "    print('Italian (true): ', target_texts[seq_index][1:-1])\n",
    "    print('Italian (pred): ', decoded_sentence[0:-1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SjF61ckSmWMj"
   },
   "source": [
    "### 4.2. Translate an English sentence to the target language\n",
    "\n",
    "1. Tokenization\n",
    "2. One-hot encode\n",
    "3. Translate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "kdIKHZxDmWMk",
    "outputId": "0f26608e-7e39-4804-e455-0648a96ae54e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source sentence is: I love you\n",
      "translated sentence is: sono qua\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_sentence = 'I love you'\n",
    "\n",
    "input_sequence = text2sequences(len(input_sentence), input_sentence)[0]\n",
    "\n",
    "input_x = onehot_encode(input_sequence, len(input_sentence), num_encoder_tokens)\n",
    "\n",
    "translated_sentence = decode_sequence(input_x)\n",
    "\n",
    "print('source sentence is: ' + input_sentence)\n",
    "print('translated sentence is: ' + translated_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bcMmrOK-mWMk"
   },
   "source": [
    "## 5. Evaluate the translation using BLEU score\n",
    "\n",
    "Reference: \n",
    "- https://machinelearningmastery.com/calculate-bleu-score-for-text-python/\n",
    "- https://en.wikipedia.org/wiki/BLEU\n",
    "\n",
    "\n",
    "**Hint:** \n",
    "\n",
    "- Randomly partition the dataset to training, validation, and test. \n",
    "\n",
    "- Evaluate the BLEU score using the test set. Report the average.\n",
    "\n",
    "- A reasonable BLEU score should be 0.1 ~ 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Vpei53h_Rzq5"
   },
   "outputs": [],
   "source": [
    "def decode_output(input_sentence):\n",
    "  input_sequence = text2sequences(len(input_sentence), input_sentence)[0]\n",
    "  input_x = onehot_encode(input_sequence, len(input_sentence), num_encoder_tokens)\n",
    "  translated_sentence = decode_sequence(input_x)\n",
    "  return translated_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "wSSl5j6eXtnD",
    "outputId": "ce382831-a36e-4013-e075-02dd68dba808"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average BLEU score:  0.14583960188881623\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "cleaned_data_train = clean_data(pairs)[0:n_train-5000,0:2]\n",
    "input_train_texts, target_train_texts = cleaned_data_train[:, 0], cleaned_data_train[:, 1]\n",
    "cleaned_data_validation = clean_data(pairs)[n_train-5000:n_train,0:2]\n",
    "input_validation_texts, target_validation_texts = cleaned_data_validation[:, 0], cleaned_data_validation[:, 1]\n",
    "cleaned_data_test = clean_data(pairs)[n_train: n_train + 2000,0:2]\n",
    "input_test_texts, target_test_texts = cleaned_data_test[:, 0], cleaned_data_test[:, 1]\n",
    "bleu_score = 0\n",
    "for i, input_sentence in enumerate(input_test_texts):\n",
    "  translated_sentence = decode_output(input_sentence)\n",
    "  reference = translated_sentence.split()\n",
    "  candidate = target_test_texts[i].split()\n",
    "  score = sentence_bleu(reference, candidate)\n",
    "  bleu_score += score \n",
    "avg_score = bleu_score / len(input_test_texts)\n",
    "print(\"Average BLEU score: \",avg_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rP-uumw8gsjL"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "HM5.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
